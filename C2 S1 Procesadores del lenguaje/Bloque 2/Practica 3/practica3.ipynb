{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_dw-rksIHZPL",
        "66Ithwjq0-2r",
        "M9G7rxW20jg8",
        "s-K0GyFDYMzp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Clases Globales\n"
      ],
      "metadata": {
        "id": "_dw-rksIHZPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class miniContenedor:\n",
        "  def __init__(self):\n",
        "    self.bowPrevio = []\n",
        "    self.bowPosterior = []\n",
        "    self.sentimientosPrevios = None\n",
        "    self.sentimientosPosteriores = None\n",
        "    self.posTaggingPre = []\n",
        "    self.posTaggingPost = []\n",
        "    self.nerPrevio = []\n",
        "    self.nerPosterior = []\n",
        "    self.textoProcesado = []\n",
        "    self.tiempoProcesado = None"
      ],
      "metadata": {
        "id": "cUzS1yTdHXoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class almacen_datos:\n",
        "    # texto1 = miniContenedor()\n",
        "    # texto2 = miniContenedor()\n",
        "    # conversacion1 = miniContenedor()\n",
        "    # conversacion2 = miniContenedor()\n",
        "\n",
        "  def __init__(self):\n",
        "    self.texto1 = miniContenedor()\n",
        "    self.texto2 = miniContenedor()\n",
        "    self.conversacion1 = miniContenedor()\n",
        "    self.conversacion2 = miniContenedor()\n",
        "\n",
        "  def _analizarNERNLTK(self,arbol):\n",
        "    for mini_arbol in arbol:\n",
        "      if isinstance(mini_arbol, nltk.Tree):\n",
        "          tipo = mini_arbol.label()\n",
        "          nombre = \" \".join(palabra for palabra, pos in mini_arbol.leaves())\n",
        "          print(f\"{nombre} ({tipo})\")\n",
        "      else:\n",
        "          palabra,_ = mini_arbol\n",
        "          print(palabra, end=' ')\n",
        "    print()\n",
        "\n",
        "  def printBowPrevio(self):\n",
        "    print(self.texto1.bowPrevio)\n",
        "    print()\n",
        "    print(self.texto2.bowPrevio)\n",
        "    print()\n",
        "    print(self.conversacion1.bowPrevio)\n",
        "    print()\n",
        "    print(self.conversacion2.bowPrevio)\n",
        "    print()\n",
        "\n",
        "  def printBowPosterior(self):\n",
        "    print(self.texto1.bowPosterior)\n",
        "    print()\n",
        "    print(self.texto2.bowPosterior)\n",
        "    print()\n",
        "    print(self.conversacion1.bowPosterior)\n",
        "    print()\n",
        "    print(self.conversacion2.bowPosterior)\n",
        "\n",
        "\n",
        "  def imprimeTodosLosBows(self):\n",
        "    self.printBowPrevio()\n",
        "    print()\n",
        "    self.printBowPosterior()\n",
        "\n",
        "  def imprimeNersNLTK(self):\n",
        "    print(\"1\")\n",
        "    self._analizarNERNLTK(self.texto1.nerPrevio)\n",
        "    print()\n",
        "    print(\"2\")\n",
        "    self._analizarNERNLTK(self.texto2.nerPrevio)\n",
        "    print()\n",
        "    print(\"3\")\n",
        "    self._analizarNERNLTK(self.conversacion1.nerPrevio)\n",
        "    print()\n",
        "    print(\"4\")\n",
        "    self._analizarNERNLTK(self.conversacion2.nerPrevio)\n",
        "    print()\n",
        "    print()\n",
        "    print(\"2-1\")\n",
        "    self._analizarNERNLTK(self.texto1.nerPosterior)\n",
        "    print()\n",
        "    print(\"2-2\")\n",
        "    self._analizarNERNLTK(self.texto2.nerPosterior)\n",
        "    print()\n",
        "    print(\"2-3\")\n",
        "    self._analizarNERNLTK(self.conversacion1.nerPosterior)\n",
        "    print()\n",
        "    print(\"2-4\")\n",
        "    self._analizarNERNLTK(self.conversacion2.nerPosterior)\n",
        "\n",
        "  def textosProcesados2file(self,nombre):\n",
        "    File = open(\"textosProcesados\"+nombre+\".txt\", \"w\")\n",
        "    File.write(self.texto1.textoProcesado+\"\\n\")\n",
        "    File.write(self.texto2.textoProcesado+\"\\n\")\n",
        "    File.write(self.conversacion1.textoProcesado+\"\\n\")\n",
        "    File.write(self.conversacion2.textoProcesado+\"\\n\")\n",
        "    File.close()\n",
        "\n",
        "  def imprimeSentimientos(self):\n",
        "    print(self.texto1.sentimientosPrevios)\n",
        "    print(self.texto2.sentimientosPrevios)\n",
        "    print(self.conversacion1.sentimientosPrevios)\n",
        "    print(self.conversacion2.sentimientosPrevios)\n",
        "\n",
        "    print()\n",
        "\n",
        "    print(self.texto1.sentimientosPosteriores)\n",
        "    print(self.texto2.sentimientosPosteriores)\n",
        "    print(self.conversacion1.sentimientosPosteriores)\n",
        "    print(self.conversacion2.sentimientosPosteriores)\n",
        "\n",
        "\n",
        "  def imprimePosTagging(self):\n",
        "    print(self.texto1.posTaggingPre)\n",
        "    print(self.texto2.posTaggingPre)\n",
        "    print(self.conversacion1.posTaggingPre)\n",
        "    print(self.conversacion2.posTaggingPre)\n",
        "    print()\n",
        "    print(self.texto1.posTaggingPost)\n",
        "    print(self.texto2.posTaggingPost)\n",
        "    print(self.conversacion1.posTaggingPost)\n",
        "    print(self.conversacion2.posTaggingPost)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D95tDBxE2249"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Constantes globales"
      ],
      "metadata": {
        "id": "66Ithwjq0-2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG_ACTIVO = False"
      ],
      "metadata": {
        "id": "umFHEsx91Dh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fragmentoElIdiota1= \"\"\"Colia tookw the prince to a public-house in the Litaynaya, not far off.\n",
        "In one of the side rooms there sat at a table—looking like one of the\n",
        "regular guests of the establishment Ardalion Alexandrovitch, with a\n",
        "bottle before him, and a newspaper on his knee. He was waiting for the\n",
        "prince, and no sooner did the latter appear than he began a long\n",
        "harangue about something or other; but so far gone was he that the\n",
        "prince could hardly understand a word.\"\"\"\n",
        "\n",
        "fragmentoElIdiota2 = \"\"\"But why recall all this? There was insanity on both sides. For him, the\n",
        "prince, to love this woman with passion, was unthinkable. It would be\n",
        "cruel and inhuman. Yes. Rogojin is not fair to himself; he has a large\n",
        "heart; he has aptitude for sympathy. When he learns the truth, and\n",
        "finds what a pitiable being is this injured, broken, half-insane\n",
        "creature, he will forgive her all the torment she has caused him. He\n",
        "will become her slave, her brother, her friend. Compassion will teach\n",
        "even Rogojin, it will show him how to reason. Compassion is the chief\n",
        "law of human existence. Oh, how guilty he felt towards Rogojin! And,\n",
        "for a few warm, hasty words spoken in Moscow, Parfen had called him\n",
        "“brother,” while he—but no, this was delirium! It would all come right!\n",
        "That gloomy Parfen had implied that his faith was waning; he must\n",
        "suffer dreadfully. He said he liked to look at that picture; it was not\n",
        "that he liked it, but he felt the need of looking at it. Rogojin was\n",
        "not merely a passionate soul; he was a fighter. He was fighting for the\n",
        "restoration of his dying faith. He must have something to hold on to\n",
        "and believe, and someone to believe in. What a strange picture that of\n",
        "Holbein’s is! Why, this is the street, and here’s the house, No. 16.\"\"\"\n",
        "\n",
        "conversacionElIdiota1 = \"\"\"The prince pulled a letter out of his pocket.\n",
        "\n",
        "“Is he raving?” said the general. “Are we really in a mad-house?”\n",
        "\n",
        "There was silence for a moment. Then Ptitsin spoke.\"\"\"\n",
        "\n",
        "conversacionElIdiota2 = \"\"\"“Bravo!” said Ferdishenko. Ptitsin laughed too, though he had been very\n",
        "sorry to see the general appear. Even Colia laughed and said, “Bravo!”\n",
        "\n",
        "“And I was right, truly right,” cried the general, with warmth and\n",
        "solemnity, “for if cigars are forbidden in railway carriages, poodles\n",
        "are much more so.”\n",
        "\n",
        "“Well, and what did the lady do?” asked Nastasia, impatiently.\n",
        "\n",
        "“She—ah, that’s where all the mischief of it lies!” replied Ivolgin,\n",
        "frowning. “Without a word, as it were, of warning, she slapped me on\n",
        "the cheek! An extraordinary woman!”\"\"\""
      ],
      "metadata": {
        "id": "trPZPoHq1Ind"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLTK"
      ],
      "metadata": {
        "id": "ZaEEQRsl0vl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación de NLTK y Otros Componentes Relacionados\n"
      ],
      "metadata": {
        "id": "M9G7rxW20jg8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9of6MOdIz8O_"
      },
      "outputs": [],
      "source": [
        "#Instalación --> NLTK y Relacionados\n",
        "!pip install nltk\n",
        "!pip install autocorrect\n",
        "!pip install svgling\n",
        "!pip install engineering_notation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importo NLTK y descargo componentes relacionados\n",
        "import nltk\n",
        "from nltk import RegexpTokenizer, WordNetLemmatizer\n",
        "nltk.download('punkt',quiet=True)\n",
        "nltk.download('punkt_tab',quiet=True)\n",
        "nltk.download('stopwords',quiet=True)\n",
        "nltk.download('wordnet',quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng',quiet=True)\n",
        "nltk.download('vader_lexicon',quiet=True)\n",
        "nltk.download('maxent_ne_chunker_tab', quiet=True)\n",
        "nltk.download('words',quiet=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XlYoaY971ZuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autocorrect import Speller"
      ],
      "metadata": {
        "id": "fdjdFSpb1WXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constantes para NLTK"
      ],
      "metadata": {
        "id": "I-xp8S0Z2EoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG_ACTIVO_NLTK = False"
      ],
      "metadata": {
        "id": "FIVXUfsS2I-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones Y Clases"
      ],
      "metadata": {
        "id": "PXobZPeK1mHh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W418Tnho2hnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizacion_nltk(_texto, paraNER=False,devolverTextoUnido=False):\n",
        "    if DEBUG_ACTIVO or DEBUG_ACTIVO_NLTK:\n",
        "        print(\"Texto original: \", _texto)\n",
        "\n",
        "    # Lowercasing (incluido en el corrector)\n",
        "    # Eliminación de signos de puntuación\n",
        "    regex_tokenizador = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "    #Corrección ortográfica (externo)\n",
        "    _texto = Speller(lang='en')(_texto)\n",
        "    if DEBUG_ACTIVO or DEBUG_ACTIVO_NLTK:\n",
        "      print(\"Corrección ortográfica: \", _texto)\n",
        "\n",
        "\n",
        "    # Eliminación de números\n",
        "    _texto = ''.join([i for i in _texto if not i.isdigit()])\n",
        "    if DEBUG_ACTIVO or DEBUG_ACTIVO_NLTK:\n",
        "        print(\"Eliminación de números: \", _texto)\n",
        "\n",
        "    # (Eliminar punct. y tokenizar)\n",
        "    tokens = regex_tokenizador.tokenize(_texto)\n",
        "\n",
        "    if DEBUG_ACTIVO or DEBUG_ACTIVO_NLTK:\n",
        "        print(\"Tokenización: \", tokens)\n",
        "\n",
        "    # Eliminación de stopwords\n",
        "    if(not paraNER):\n",
        "      stopwords = nltk.corpus.stopwords.words('english')\n",
        "      tokens = [token for token in tokens if token not in stopwords]\n",
        "      if DEBUG_ACTIVO or DEBUG_ACTIVO_NLTK:\n",
        "        print(\"Eliminación de stopwords: \", tokens)\n",
        "\n",
        "\n",
        "        # Lematización\n",
        "    # lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    if DEBUG_ACTIVO or DEBUG_ACTIVO_NLTK:\n",
        "        print(\"Lematización: \", tokens)\n",
        "\n",
        "    # Stemming (Incluido en NLTK) (pasa a a minusculas, por lo que no sirve para NER)\n",
        "    if(not paraNER):\n",
        "      stemmer = nltk.stem.SnowballStemmer('english')\n",
        "      tokens = [stemmer.stem(token) for token in tokens]\n",
        "      if DEBUG_ACTIVO or DEBUG_ACTIVO_NLTK:\n",
        "          print(\"Stemming: \", tokens)\n",
        "\n",
        "\n",
        "    # Lo devuelvo a texto\n",
        "    texto_normalizado_unido = \" \".join(tokens)\n",
        "\n",
        "    salida = texto_normalizado_unido if devolverTextoUnido else tokens\n",
        "\n",
        "    # Lo tokenizo\n",
        "    return salida"
      ],
      "metadata": {
        "id": "R5qLjeFT1tDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del almacén de datos"
      ],
      "metadata": {
        "id": "9ddqTJrh4qWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk = almacen_datos()"
      ],
      "metadata": {
        "id": "dN5OsM8M4p4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TestBench"
      ],
      "metadata": {
        "id": "_cIAj3fq3hDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports Necesarios\n",
        "from engineering_notation import EngNumber"
      ],
      "metadata": {
        "id": "WgCGB2z95yt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk.texto1.tiempoProcesado = %timeit -r 100 -o normalizacion_nltk(fragmentoElIdiota1)\n",
        "datos_nltk.texto2.tiempoProcesado = %timeit -r 100 -o normalizacion_nltk(fragmentoElIdiota2)\n",
        "datos_nltk.conversacion1.tiempoProcesado = %timeit -r 100 -o normalizacion_nltk(conversacionElIdiota1)\n",
        "datos_nltk.conversacion2.tiempoProcesado = %timeit -r 100 -o normalizacion_nltk(conversacionElIdiota2)"
      ],
      "metadata": {
        "id": "t_dY3SnN3jlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lo imprimo visualmente\n",
        "print(f'descripcion  1: Media:{EngNumber(datos_nltk.texto1.tiempoProcesado.average)}                     | desv_tipica: {EngNumber(datos_nltk.texto1.tiempoProcesado.stdev)}                     | Mejor: {EngNumber(datos_nltk.texto1.tiempoProcesado.best)}                     | Peor: {EngNumber(datos_nltk.texto1.tiempoProcesado.worst)}        | tiempo de compilación: {EngNumber(datos_nltk.texto1.tiempoProcesado.compile_time)}')\n",
        "print(f'descripcion  2: Media:{EngNumber(datos_nltk.texto2.tiempoProcesado.average)}                     | desv_tipica: {EngNumber(datos_nltk.texto2.tiempoProcesado.stdev)}                     | Mejor: {EngNumber(datos_nltk.texto2.tiempoProcesado.best)}                     | Peor: {EngNumber(datos_nltk.texto2.tiempoProcesado.worst)}        | tiempo de compilación: {EngNumber(datos_nltk.texto2.tiempoProcesado.compile_time)}')\n",
        "print(f'conversacion 1: Media:{EngNumber(datos_nltk.conversacion1.tiempoProcesado.average)}              | desv_tipica: {EngNumber(datos_nltk.conversacion1.tiempoProcesado.stdev)}              | Mejor: {EngNumber(datos_nltk.conversacion1.tiempoProcesado.best)}              | Peor: {EngNumber(datos_nltk.conversacion1.tiempoProcesado.worst)} | tiempo de compilación: {EngNumber(datos_nltk.conversacion1.tiempoProcesado.compile_time)}')\n",
        "print(f'conversacion 2: Media:{EngNumber(datos_nltk.conversacion2.tiempoProcesado.average)}              | desv_tipica: {EngNumber(datos_nltk.conversacion2.tiempoProcesado.stdev)}              | Mejor: {EngNumber(datos_nltk.conversacion2.tiempoProcesado.best)}              | Peor: {EngNumber(datos_nltk.conversacion2.tiempoProcesado.worst)} | tiempo de compilación: {EngNumber(datos_nltk.conversacion2.tiempoProcesado.compile_time)}')"
      ],
      "metadata": {
        "id": "vIw9Xsu25o8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salida = [\n",
        "           [\"doc1\",datos_nltk.texto1.tiempoProcesado.all_runs]\n",
        "          ,[\"doc2\",datos_nltk.texto2.tiempoProcesado.all_runs]\n",
        "          ,[\"conversacion1\",datos_nltk.conversacion1.tiempoProcesado.all_runs]\n",
        "          ,[\"conversacion2\",datos_nltk.conversacion2.tiempoProcesado.all_runs]\n",
        "          ]\n",
        "print(salida)"
      ],
      "metadata": {
        "id": "mLiPVIko7OHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag Of Words (BoW)"
      ],
      "metadata": {
        "id": "u0b7g6oV7kMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hago Imports\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "QvCswPYV7vhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk.texto1.bowPrevio = [(word,freq) for word,freq in FreqDist(fragmentoElIdiota1.split()).items()]\n",
        "datos_nltk.texto1.bowPosterior = [(word,freq) for word,freq in FreqDist(normalizacion_nltk(fragmentoElIdiota1)).items()]\n",
        "\n",
        "datos_nltk.texto2.bowPrevio = [(word,freq) for word,freq in FreqDist(fragmentoElIdiota2.split()).items()]\n",
        "datos_nltk.texto2.bowPosterior = [(word,freq) for word,freq in FreqDist(normalizacion_nltk(fragmentoElIdiota2)).items()]\n",
        "\n",
        "datos_nltk.conversacion1.bowPrevio = [(word,freq) for word,freq in FreqDist(conversacionElIdiota1.split()).items()]\n",
        "datos_nltk.conversacion1.bowPosterior = [(word,freq) for word,freq in FreqDist(normalizacion_nltk(conversacionElIdiota1)).items()]\n",
        "\n",
        "datos_nltk.conversacion2.bowPrevio = [(word,freq) for word,freq in FreqDist(conversacionElIdiota2.split()).items()]\n",
        "datos_nltk.conversacion2.bowPosterior = [(word,freq) for word,freq in FreqDist(normalizacion_nltk(conversacionElIdiota2)).items()]\n"
      ],
      "metadata": {
        "id": "mWDDD6247qkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk.imprimeTodosLosBows()"
      ],
      "metadata": {
        "id": "nSon10Ri7-Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis de sentimientos"
      ],
      "metadata": {
        "id": "GIRN0jvI79xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importo librerías\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
      ],
      "metadata": {
        "id": "dDSvB85m8PaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentimientos.texto1        = SentimentIntensityAnalyzer().polarity_scores(pruebaElIdiota1)\n",
        "datos_nltk.texto1.sentimientosPrevios = SentimentIntensityAnalyzer().polarity_scores(fragmentoElIdiota1)\n",
        "datos_nltk.texto1.sentimientosPosteriores = SentimentIntensityAnalyzer().polarity_scores(\" \".join(normalizacion_nltk(fragmentoElIdiota1)))\n",
        "\n",
        "datos_nltk.texto2.sentimientosPrevios = SentimentIntensityAnalyzer().polarity_scores(fragmentoElIdiota2)\n",
        "datos_nltk.texto2.sentimientosPosteriores = SentimentIntensityAnalyzer().polarity_scores(\" \".join(normalizacion_nltk(fragmentoElIdiota2)))\n",
        "\n",
        "datos_nltk.conversacion1.sentimientosPrevios = SentimentIntensityAnalyzer().polarity_scores(conversacionElIdiota1)\n",
        "datos_nltk.conversacion1.sentimientosPosteriores = SentimentIntensityAnalyzer().polarity_scores(\" \".join(normalizacion_nltk(conversacionElIdiota1)))\n",
        "\n",
        "datos_nltk.conversacion2.sentimientosPrevios = SentimentIntensityAnalyzer().polarity_scores(conversacionElIdiota2)\n",
        "datos_nltk.conversacion2.sentimientosPosteriores = SentimentIntensityAnalyzer().polarity_scores(\" \".join(normalizacion_nltk(conversacionElIdiota2)))"
      ],
      "metadata": {
        "id": "pvSdHpvT8Ul_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk.imprimeSentimientos()"
      ],
      "metadata": {
        "id": "d6HKND1F9Zlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part Of Speech (P.O.S. Tagging)"
      ],
      "metadata": {
        "id": "HP_LWjcg82r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk.texto1.posTaggingPre = nltk.pos_tag(word_tokenize(fragmentoElIdiota1))\n",
        "datos_nltk.texto1.posTaggingPost = nltk.pos_tag(normalizacion_nltk(fragmentoElIdiota1))\n",
        "\n",
        "datos_nltk.texto2.posTaggingPre = nltk.pos_tag(word_tokenize(fragmentoElIdiota2))\n",
        "datos_nltk.texto2.posTaggingPost = nltk.pos_tag(normalizacion_nltk(fragmentoElIdiota2))\n",
        "\n",
        "datos_nltk.conversacion1.posTaggingPre = nltk.pos_tag(word_tokenize(conversacionElIdiota1))\n",
        "datos_nltk.conversacion1.posTaggingPost = nltk.pos_tag(normalizacion_nltk(conversacionElIdiota1))\n",
        "\n",
        "datos_nltk.conversacion2.posTaggingPre = nltk.pos_tag(word_tokenize(conversacionElIdiota2))\n",
        "datos_nltk.conversacion2.posTaggingPost = nltk.pos_tag(normalizacion_nltk(conversacionElIdiota2))"
      ],
      "metadata": {
        "id": "d_iFtA6-93wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimo\n",
        "print(datos_nltk.texto1.posTaggingPre)\n",
        "print()\n",
        "print(datos_nltk.texto1.posTaggingPost)\n",
        "print()\n",
        "print(datos_nltk.texto2.posTaggingPre)\n",
        "print()\n",
        "print(datos_nltk.texto2.posTaggingPost)\n",
        "print()\n",
        "print(datos_nltk.conversacion1.posTaggingPre)\n",
        "print()\n",
        "print(datos_nltk.conversacion1.posTaggingPost)\n",
        "print()\n",
        "print(datos_nltk.conversacion2.posTaggingPre)\n",
        "print()\n",
        "print(datos_nltk.conversacion2.posTaggingPost)"
      ],
      "metadata": {
        "id": "hitlqCg1-nTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "yT36hMyB-xcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk.texto1.nerPrevio = nltk.ne_chunk(nltk.pos_tag(word_tokenize(fragmentoElIdiota1)))\n",
        "datos_nltk.texto1.nerPosterior = nltk.ne_chunk(nltk.pos_tag(normalizacion_nltk(fragmentoElIdiota1,paraNER=True)))\n",
        "\n",
        "datos_nltk.texto2.nerPrevio = nltk.ne_chunk(nltk.pos_tag(word_tokenize(fragmentoElIdiota2)))\n",
        "datos_nltk.texto2.nerPosterior = nltk.ne_chunk(nltk.pos_tag(normalizacion_nltk(fragmentoElIdiota2,paraNER=True)))\n",
        "\n",
        "datos_nltk.conversacion1.nerPrevio = nltk.ne_chunk(nltk.pos_tag(word_tokenize(conversacionElIdiota1)))\n",
        "datos_nltk.conversacion1.nerPosterior = nltk.ne_chunk(nltk.pos_tag(normalizacion_nltk(conversacionElIdiota1,paraNER=True)))\n",
        "\n",
        "datos_nltk.conversacion2.nerPrevio = nltk.ne_chunk(nltk.pos_tag(word_tokenize(conversacionElIdiota2)))\n",
        "datos_nltk.conversacion2.nerPosterior = nltk.ne_chunk(nltk.pos_tag(normalizacion_nltk(conversacionElIdiota2,paraNER=True)))"
      ],
      "metadata": {
        "id": "PTFRpxny-24_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk.imprimeNersNLTK()"
      ],
      "metadata": {
        "id": "md-dOZQ5_OlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultado de procesamiento como cadena"
      ],
      "metadata": {
        "id": "LkhXMC66_Up-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk.texto1.textoProcesado = normalizacion_nltk(fragmentoElIdiota1,devolverTextoUnido=True)\n",
        "datos_nltk.texto2.textoProcesado = normalizacion_nltk(fragmentoElIdiota2,devolverTextoUnido=True)\n",
        "datos_nltk.conversacion1.textoProcesado = normalizacion_nltk(conversacionElIdiota1,devolverTextoUnido=True)\n",
        "datos_nltk.conversacion2.textoProcesado = normalizacion_nltk(conversacionElIdiota2,devolverTextoUnido=True)"
      ],
      "metadata": {
        "id": "HjX8V9XM_UYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_nltk.textosProcesados2file(\"NLTK\")"
      ],
      "metadata": {
        "id": "57jEpEgB_nge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SpaCy"
      ],
      "metadata": {
        "id": "m5-dk1tWHlor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación de SpaCy y Otros Componentes Relacionados"
      ],
      "metadata": {
        "id": "cT8GCaxxIDKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install contextualSpellCheck\n",
        "\n",
        "!pip install engineering_notation\n",
        "!pip install spacytextblob"
      ],
      "metadata": {
        "id": "xL2_mBoyHySH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "7LLSnD81H6F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constantes"
      ],
      "metadata": {
        "id": "XPftBLnTIwJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG_SPACY = False"
      ],
      "metadata": {
        "id": "3wHMcruFIySN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones y Clases"
      ],
      "metadata": {
        "id": "kjHLe07vIGpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import re\n",
        "import contextualSpellCheck"
      ],
      "metadata": {
        "id": "8VCXVTjIIKN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizacion_spacy(text,nlp,lematizar,obtener_doc=False, obtener_texto=False):\n",
        "    DEBUG_HABILITADO = False\n",
        "    # Convert to lowercase\n",
        "    texto_minusculas = text.lower()\n",
        "    if(DEBUG_HABILITADO or DEBUG_SPACY):\n",
        "        print(f'Texto en minúsculas: {texto_minusculas}\\n')\n",
        "\n",
        "    # Corrector ortográfico\n",
        "    tuberias_desactivadas = nlp.disable_pipes(\"tagger\", \"lemmatizer\")#desactivo todas las no esenciales para spelcheck\n",
        "\n",
        "    if(not nlp.has_pipe(\"contextual spellchecker\")):\n",
        "        contextualSpellCheck.add_to_pipe(nlp)\n",
        "        nlp.enable_pipe(\"contextual spellchecker\")\n",
        "\n",
        "\n",
        "    if(DEBUG_HABILITADO or DEBUG_SPACY):\n",
        "        print(f'Texto a corregir: {texto_minusculas}')\n",
        "\n",
        "    doc_corregido = list(nlp.pipe([texto_minusculas]))\n",
        "    if(DEBUG_HABILITADO or DEBUG_SPACY):\n",
        "        for(token) in doc_corregido[0]:\n",
        "            print(f'token: {token.text} | token_corregido: {token._.get_require_spellCheck}')\n",
        "\n",
        "    texto_corregido = ' '.join([token._.get_suggestion_spellCheck  if token._.get_suggestion_spellCheck!=''else token.text for token in doc_corregido[0] ])\n",
        "\n",
        "    if(DEBUG_HABILITADO or DEBUG_SPACY):\n",
        "        print(f'\\nPOSTPIPE Texto corregido: {texto_corregido}\\n')\n",
        "\n",
        "\n",
        "\n",
        "    nlp.remove_pipe(\"contextual spellchecker\")\n",
        "\n",
        "    nlp.enable_pipe(\"lemmatizer\")\n",
        "    nlp.enable_pipe(\"tagger\")\n",
        "\n",
        "    doc = list(nlp.pipe([texto_corregido]))\n",
        "\n",
        "    normalized_tokens = []\n",
        "\n",
        "\n",
        "    for token in doc[0]:\n",
        "        # Stop words\n",
        "        if token.text in STOP_WORDS:\n",
        "            continue\n",
        "\n",
        "        # Puntuacion, espacios y dígitos\n",
        "        if token.is_space or token.is_punct or token.is_digit:\n",
        "            continue\n",
        "\n",
        "        normalized_tokens.append(token)\n",
        "\n",
        "    # texto normalizado\n",
        "    normalized_text = ' '.join([token.lemma_ for token in normalized_tokens])\n",
        "\n",
        "\n",
        "    if(obtener_texto):\n",
        "      return normalized_text\n",
        "\n",
        "    if(obtener_doc):\n",
        "      return doc[0]\n",
        "\n",
        "    if(lematizar):\n",
        "        return [token.lemma_ for token in normalized_tokens]\n",
        "    else:\n",
        "        return normalized_tokens"
      ],
      "metadata": {
        "id": "doKRDmnRIPO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del almacen de datos"
      ],
      "metadata": {
        "id": "lSqFREGsIJef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_spacy = almacen_datos()"
      ],
      "metadata": {
        "id": "DqdPigLAI-sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TestBench"
      ],
      "metadata": {
        "id": "cDQN1EXiJB2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_spacy.texto1.tiempoProcesado        = %timeit -r 100 -o normalizacion_spacy(fragmentoElIdiota1    , nlp, True)\n",
        "datos_spacy.texto2.tiempoProcesado        = %timeit -r 100 -o normalizacion_spacy(fragmentoElIdiota2    , nlp, True)\n",
        "datos_spacy.conversacion1.tiempoProcesado = %timeit -r 100 -o normalizacion_spacy(conversacionElIdiota1 , nlp, True)\n",
        "datos_spacy.conversacion2.tiempoProcesado = %timeit -r 100 -o normalizacion_spacy(conversacionElIdiota2 , nlp, True)"
      ],
      "metadata": {
        "id": "nhhYQ7xwJGOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from engineering_notation import EngNumber\n",
        "print(f'descripcion 1: Media:{EngNumber(datos_spacy.texto1.tiempoProcesado.average)}                     | desv_tipica: {EngNumber(datos_spacy.texto1.tiempoProcesado.stdev)}                     | Mejor: {EngNumber(datos_spacy.texto1.tiempoProcesado.best)}                     | Peor: {EngNumber(datos_spacy.texto1.tiempoProcesado.worst)}        | tiempo de compilación: {EngNumber(datos_spacy.texto1.tiempoProcesado.compile_time)}')\n",
        "print(f'descripcion 2: Media:{EngNumber(datos_spacy.texto2.tiempoProcesado.average)}                     | desv_tipica: {EngNumber(datos_spacy.texto2.tiempoProcesado.stdev)}                     | Mejor: {EngNumber(datos_spacy.texto2.tiempoProcesado.best)}                     | Peor: {EngNumber(datos_spacy.texto2.tiempoProcesado.worst)}        | tiempo de compilación: {EngNumber(datos_spacy.texto2.tiempoProcesado.compile_time)}')\n",
        "print(f'conversacion 1: Media:{EngNumber(datos_spacy.conversacion1.tiempoProcesado.average)}              | desv_tipica: {EngNumber(datos_spacy.conversacion1.tiempoProcesado.stdev)}             | Mejor: {EngNumber(datos_spacy.conversacion1.tiempoProcesado.best)}             | Peor: {EngNumber(datos_spacy.conversacion1.tiempoProcesado.worst)}  | tiempo de compilación: {EngNumber(datos_spacy.conversacion1.tiempoProcesado.compile_time)}')\n",
        "print(f'conversacion 2: Media:{EngNumber(datos_spacy.conversacion2.tiempoProcesado.average)}              | desv_tipica: {EngNumber(datos_spacy.conversacion2.tiempoProcesado.stdev)}             | Mejor: {EngNumber(datos_spacy.conversacion2.tiempoProcesado.best)}             | Peor: {EngNumber(datos_spacy.conversacion2.tiempoProcesado.worst)}  | tiempo de compilación: {EngNumber(datos_spacy.conversacion2.tiempoProcesado.compile_time)}')"
      ],
      "metadata": {
        "id": "RlyoxxXhJzzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salida = [\n",
        "    [\"doc1\",datos_spacy.texto1.tiempoProcesado.all_runs]\n",
        "          ,[\"doc2\",datos_spacy.texto2.tiempoProcesado.all_runs]\n",
        "          ,[\"conversacion1\",datos_spacy.conversacion1.tiempoProcesado.all_runs]\n",
        "          ,[\"conversacion2\",datos_spacy.conversacion2.tiempoProcesado.all_runs]\n",
        "          ]\n",
        "\n",
        "print(salida)"
      ],
      "metadata": {
        "id": "sz1CJwWGLCGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag Of Words (BoW)"
      ],
      "metadata": {
        "id": "g7jqHpG-OqLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "datos_spacy.texto1.bowPrevio    = dict( Counter( fragmentoElIdiota1.split() ) )\n",
        "datos_spacy.texto1.bowPosterior = dict( Counter( normalizacion_spacy(fragmentoElIdiota1, nlp, lematizar=True) ) )\n",
        "\n",
        "datos_spacy.texto2.bowPrevio    = dict( Counter( fragmentoElIdiota2.split() ) )\n",
        "datos_spacy.texto2.bowPosterior = dict( Counter( normalizacion_spacy(fragmentoElIdiota2, nlp, lematizar=True) ) )\n",
        "\n",
        "datos_spacy.conversacion1.bowPrevio    = dict( Counter( conversacionElIdiota1.split() ) )\n",
        "datos_spacy.conversacion1.bowPosterior = dict( Counter( normalizacion_spacy(conversacionElIdiota1, nlp, lematizar=True) ) )\n",
        "\n",
        "datos_spacy.conversacion2.bowPrevio    = dict( Counter( conversacionElIdiota2.split() ) )\n",
        "datos_spacy.conversacion2.bowPosterior = dict( Counter( normalizacion_spacy(conversacionElIdiota2, nlp, lematizar=True) ) )"
      ],
      "metadata": {
        "id": "pvj2czIrLXKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_spacy.imprimeTodosLosBows()"
      ],
      "metadata": {
        "id": "r6wlc-Z9PX38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER"
      ],
      "metadata": {
        "id": "HDJDQfnbPbkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(list(nlp.pipe([fragmentoElIdiota1])), style=\"ent\", jupyter=True)\n",
        "print()\n",
        "displacy.render(normalizacion_spacy(fragmentoElIdiota1,nlp,True,True), style=\"ent\", jupyter=True)\n",
        "print()\n",
        "print()\n",
        "displacy.render(list(nlp.pipe([fragmentoElIdiota2])), style=\"ent\", jupyter=True)\n",
        "print()\n",
        "displacy.render(normalizacion_spacy(fragmentoElIdiota2,nlp,True,True), style=\"ent\", jupyter=True)\n",
        "print()\n",
        "print()\n",
        "displacy.render(list(nlp.pipe([conversacionElIdiota1])), style=\"ent\", jupyter=True)\n",
        "print()\n",
        "displacy.render(normalizacion_spacy(conversacionElIdiota1,nlp,True,True), style=\"ent\", jupyter=True)\n",
        "print()\n",
        "print()\n",
        "displacy.render(list(nlp.pipe([conversacionElIdiota2])), style=\"ent\", jupyter=True)\n",
        "print()\n",
        "displacy.render(normalizacion_spacy(conversacionElIdiota2,nlp,True,True), style=\"ent\", jupyter=True)\n",
        "print()"
      ],
      "metadata": {
        "id": "EHIscjomPiIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisis de sentimientos"
      ],
      "metadata": {
        "id": "Nrn0ehDWQLB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlpDefecto = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "X5JxJPmoQWpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "if(not nlp.has_pipe(\"spacytextblob\")):\n",
        "    nlp.add_pipe('spacytextblob')\n",
        "if(not nlpDefecto.has_pipe(\"spacytextblob\")):\n",
        "    nlpDefecto.add_pipe('spacytextblob')\n",
        "\n",
        "doc = normalizacion_spacy(fragmentoElIdiota1,nlp,False,True)\n",
        "print(\"polaridad:\",doc._.blob.polarity)\n",
        "print(\"subjetividad:\",doc._.blob.subjectivity)\n",
        "print(\"assessments:\",doc._.blob.sentiment_assessments.assessments)\n",
        "print()\n",
        "doc = nlpDefecto(fragmentoElIdiota1)\n",
        "print(\"polaridad:\",doc._.blob.polarity)\n",
        "print(\"subjetividad:\",doc._.blob.subjectivity)\n",
        "print(\"assessments:\",doc._.blob.sentiment_assessments.assessments)\n",
        "print()\n",
        "print()\n",
        "doc = normalizacion_spacy(fragmentoElIdiota2,nlp,False,True)\n",
        "print(\"polaridad:\",doc._.blob.polarity)\n",
        "print(\"subjetividad:\",doc._.blob.subjectivity)\n",
        "print(\"assessments:\",doc._.blob.sentiment_assessments.assessments)\n",
        "print()\n",
        "doc = nlpDefecto(fragmentoElIdiota2)\n",
        "print(\"polaridad:\",doc._.blob.polarity)\n",
        "print(\"subjetividad:\",doc._.blob.subjectivity)\n",
        "print(\"assessments:\",doc._.blob.sentiment_assessments.assessments)\n",
        "print()\n",
        "print()\n",
        "doc = normalizacion_spacy(conversacionElIdiota1,nlp,False,True)\n",
        "print(\"polaridad:\",doc._.blob.polarity)\n",
        "print(\"subjetividad:\",doc._.blob.subjectivity)\n",
        "print(\"assessments:\",doc._.blob.sentiment_assessments.assessments)\n",
        "print()\n",
        "doc = nlpDefecto(conversacionElIdiota1)\n",
        "print(\"polaridad:\",doc._.blob.polarity)\n",
        "print(\"subjetividad:\",doc._.blob.subjectivity)\n",
        "print(\"assessments:\",doc._.blob.sentiment_assessments.assessments)\n",
        "print()\n",
        "print()\n",
        "doc = normalizacion_spacy(conversacionElIdiota2,nlp,False,True)\n",
        "print(\"polaridad:\",doc._.blob.polarity)\n",
        "print(\"subjetividad:\",doc._.blob.subjectivity)\n",
        "print(\"assessments:\",doc._.blob.sentiment_assessments.assessments)\n",
        "print()\n",
        "doc = nlpDefecto(conversacionElIdiota2)\n",
        "print(\"polaridad:\",doc._.blob.polarity)\n",
        "print(\"subjetividad:\",doc._.blob.subjectivity)\n",
        "print(\"assessments:\",doc._.blob.sentiment_assessments.assessments)\n",
        "\n",
        "\n",
        "nlp.remove_pipe(\"spacytextblob\")\n",
        "nlpDefecto.remove_pipe(\"spacytextblob\")"
      ],
      "metadata": {
        "id": "e5nrS3nDQNqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS tagging"
      ],
      "metadata": {
        "id": "CRsiEjGKQqFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pos Tagging\n",
        "from spacy import displacy\n",
        "doc = normalizacion_spacy(fragmentoElIdiota1,nlp,False,True)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "print()\n",
        "doc = nlpDefecto(fragmentoElIdiota1)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "\n",
        "print(\"------\")\n",
        "\n",
        "doc = normalizacion_spacy(fragmentoElIdiota2,nlp,False,True)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "print()\n",
        "doc = nlpDefecto(fragmentoElIdiota2)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "\n",
        "print(\"------\")\n",
        "\n",
        "doc = normalizacion_spacy(conversacionElIdiota1,nlp,False,True)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "print()\n",
        "doc = nlpDefecto(conversacionElIdiota1)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "\n",
        "print(\"------\")\n",
        "\n",
        "doc = normalizacion_spacy(conversacionElIdiota2,nlp,False,True)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "print()\n",
        "doc = nlpDefecto(conversacionElIdiota2)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o0jgMgSwQsXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultado de procesamiento como cadena"
      ],
      "metadata": {
        "id": "RPlHgY_JQ7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_spacy.texto1.textoProcesado             = normalizacion_spacy(fragmentoElIdiota1, nlp, lematizar=True, obtener_texto=True)\n",
        "datos_spacy.texto2.textoProcesado             = normalizacion_spacy(fragmentoElIdiota2, nlp, lematizar=True, obtener_texto=True)\n",
        "datos_spacy.conversacion1.textoProcesado      = normalizacion_spacy(conversacionElIdiota1, nlp, lematizar=True, obtener_texto=True)\n",
        "datos_spacy.conversacion2.textoProcesado      = normalizacion_spacy(conversacionElIdiota2, nlp, lematizar=True, obtener_texto=True)"
      ],
      "metadata": {
        "id": "P20yLwURQ-Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_spacy.textosProcesados2file(\"SpaCy\")"
      ],
      "metadata": {
        "id": "R7pjWHsrRLM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextBlob"
      ],
      "metadata": {
        "id": "GH1vbKJzSLnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalaciones\n"
      ],
      "metadata": {
        "id": "N728juyNSYui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "!pip install engineering_notation"
      ],
      "metadata": {
        "id": "zHQX3pKcScKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab', quiet = True)\n",
        "nltk.download('wordnet', quiet = True)\n",
        "nltk.download('stopwords', quiet = True)"
      ],
      "metadata": {
        "id": "vMWh--EoSdxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones y Clases"
      ],
      "metadata": {
        "id": "iGZxLJNDSosC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "\n",
        "def normalizar_textBlob(texto, devolverBlob=False, devolverCadena=False):\n",
        "  # Elimino Números\n",
        "  textoSinNumeros = [word for word in texto if not word.isdigit()]\n",
        "\n",
        "  texto2 = ''.join(textoSinNumeros)\n",
        "\n",
        "  # print(texto2)\n",
        "\n",
        "\n",
        "  # Puntuación (ya incluída)\n",
        "  # Segmentación en palabras (Incluído en TextBlob)\n",
        "  documento = TextBlob(texto2)\n",
        "\n",
        "  # Lowercasing (incluído en TextBlob)\n",
        "  tokensEnMinusculas = documento.correct().words.lower()\n",
        "\n",
        "\n",
        "  # Lemma\n",
        "  tokensLematizados = tokensEnMinusculas.lemmatize()\n",
        "\n",
        "\n",
        "  # print(tokensLematizados)\n",
        "\n",
        "  # Stemming\n",
        "  tokensStemmeados = tokensLematizados.stem()\n",
        "\n",
        "  # print(tokensStemmeados)\n",
        "\n",
        "\n",
        "  tokensParaFiltrar = tokensStemmeados\n",
        "\n",
        "  # Eliminación de stopwords\n",
        "  filtered_tokens = [word for word in tokensParaFiltrar if word not in nltk.corpus.stopwords.words('english')]\n",
        "\n",
        "\n",
        "  if(devolverCadena):\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "  if(devolverBlob):\n",
        "    return TextBlob(\" \".join(filtered_tokens))\n",
        "\n",
        "  return(filtered_tokens)"
      ],
      "metadata": {
        "id": "jjFV7_X_SlNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del almacen de datos"
      ],
      "metadata": {
        "id": "jkVFda_8Svnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_textblob = almacen_datos()"
      ],
      "metadata": {
        "id": "ONpB8AEKS5LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TestBench"
      ],
      "metadata": {
        "id": "zWdo3-TRS8ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_textblob.texto1.tiempoProcesado        = %timeit -r 100 -o normalizar_textBlob(fragmentoElIdiota1)\n",
        "datos_textblob.texto2.tiempoProcesado        = %timeit -r 100 -o normalizar_textBlob(fragmentoElIdiota2)\n",
        "datos_textblob.conversacion1.tiempoProcesado = %timeit -r 100 -o normalizar_textBlob(conversacionElIdiota1)\n",
        "datos_textblob.conversacion2.tiempoProcesado = %timeit -r 100 -o normalizar_textBlob(conversacionElIdiota1)"
      ],
      "metadata": {
        "id": "o2J-IooFS_AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Descripcion 1: Media:{EngNumber(datos_textblob.texto1.tiempoProcesado.average)}                     | desv_tipica: {EngNumber(datos_textblob.texto1.tiempoProcesado.stdev)}                     | Mejor: {EngNumber(datos_textblob.texto1.tiempoProcesado.best)}                     | Peor: {EngNumber(datos_textblob.texto1.tiempoProcesado.worst)}        | tiempo de compilación: {EngNumber(datos_textblob.texto1.tiempoProcesado.compile_time)}')\n",
        "print(f'Descripcion 2: Media:{EngNumber(datos_textblob.texto2.tiempoProcesado.average)}                     | desv_tipica: {EngNumber(datos_textblob.texto2.tiempoProcesado.stdev)}                     | Mejor: {EngNumber(datos_textblob.texto2.tiempoProcesado.best)}                     | Peor: {EngNumber(datos_textblob.texto2.tiempoProcesado.worst)}        | tiempo de compilación: {EngNumber(datos_textblob.texto2.tiempoProcesado.compile_time)}')\n",
        "print(f'Conversacion 1: Media:{EngNumber(datos_textblob.conversacion1.tiempoProcesado.average)}              | desv_tipica: {EngNumber(datos_textblob.conversacion1.tiempoProcesado.stdev)}             | Mejor: {EngNumber(datos_textblob.conversacion1.tiempoProcesado.best)}             | Peor: {EngNumber(datos_textblob.conversacion1.tiempoProcesado.worst)}  | tiempo de compilación: {EngNumber(datos_textblob.conversacion1.tiempoProcesado.compile_time)}')\n",
        "print(f'Conversacion 2: Media:{EngNumber(datos_textblob.conversacion2.tiempoProcesado.average)}              | desv_tipica: {EngNumber(datos_textblob.conversacion2.tiempoProcesado.stdev)}             | Mejor: {EngNumber(datos_textblob.conversacion2.tiempoProcesado.best)})            | Peor: {EngNumber(datos_textblob.conversacion2.tiempoProcesado.worst)}  | tiempo de compilación: {EngNumber(datos_textblob.conversacion2.tiempoProcesado.compile_time)}')\n"
      ],
      "metadata": {
        "id": "yHVKkcw8TV0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "PZ3vMOi3Uv2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown', quiet = True)"
      ],
      "metadata": {
        "id": "oXPTBoS3VKMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_textblob.texto1.bowPrevio = TextBlob(fragmentoElIdiota1).word_counts\n",
        "datos_textblob.texto1.bowPosterior = normalizar_textBlob(fragmentoElIdiota1,devolverBlob=True).word_counts\n",
        "\n",
        "datos_textblob.texto2.bowPrevio = TextBlob(fragmentoElIdiota2).word_counts\n",
        "datos_textblob.texto2.bowPosterior = normalizar_textBlob(fragmentoElIdiota2,devolverBlob=True).word_counts\n",
        "\n",
        "datos_textblob.conversacion1.bowPrevio = TextBlob(conversacionElIdiota1).word_counts\n",
        "datos_textblob.conversacion1.bowPosterior = normalizar_textBlob(conversacionElIdiota1,devolverBlob=True).word_counts\n",
        "\n",
        "datos_textblob.conversacion2.bowPrevio = TextBlob(conversacionElIdiota2).word_counts\n",
        "datos_textblob.conversacion2.bowPosterior = normalizar_textBlob(conversacionElIdiota2,devolverBlob=True).word_counts\n"
      ],
      "metadata": {
        "id": "zeErKmBhUNsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_textblob.imprimeTodosLosBows()"
      ],
      "metadata": {
        "id": "CV1oed6iVrZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentimientos"
      ],
      "metadata": {
        "id": "d8YqL9JlVwHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_textblob.texto1.sentimientosPrevios = TextBlob(fragmentoElIdiota1).sentiment\n",
        "datos_textblob.texto1.sentimientosPosteriores = normalizar_textBlob(fragmentoElIdiota1,devolverBlob=True).sentiment\n",
        "\n",
        "datos_textblob.texto2.sentimientosPrevios = TextBlob(fragmentoElIdiota2).sentiment\n",
        "datos_textblob.texto2.sentimientosPosteriores = normalizar_textBlob(fragmentoElIdiota2,devolverBlob=True).sentiment\n",
        "\n",
        "datos_textblob.conversacion1.sentimientosPrevios = TextBlob(conversacionElIdiota1).sentiment\n",
        "datos_textblob.conversacion1.sentimientosPosteriores = normalizar_textBlob(conversacionElIdiota1,devolverBlob=True).sentiment\n",
        "\n",
        "datos_textblob.conversacion2.sentimientosPrevios = TextBlob(conversacionElIdiota2).sentiment\n",
        "datos_textblob.conversacion2.sentimientosPosteriores = normalizar_textBlob(conversacionElIdiota2,devolverBlob=True).sentiment"
      ],
      "metadata": {
        "id": "7SEc4ZJnV5XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_textblob.imprimeSentimientos()"
      ],
      "metadata": {
        "id": "E7cVX1S2WRuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS"
      ],
      "metadata": {
        "id": "fdR-C31TWd5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng', quiet = True)\n",
        "\n",
        "\n",
        "datos_textblob.texto1.posTaggingPre = TextBlob(fragmentoElIdiota1).pos_tags\n",
        "datos_textblob.texto1.posTaggingPosterior = normalizar_textBlob(fragmentoElIdiota1,True).pos_tags\n",
        "\n",
        "datos_textblob.texto2.posTaggingPre = TextBlob(fragmentoElIdiota2).pos_tags\n",
        "datos_textblob.texto2.posTaggingPost = normalizar_textBlob(fragmentoElIdiota2,True).pos_tags\n",
        "\n",
        "datos_textblob.conversacion1.posTaggingPre = TextBlob(conversacionElIdiota1).pos_tags\n",
        "datos_textblob.conversacion1.posTaggingPost = normalizar_textBlob(conversacionElIdiota1,True).pos_tags\n",
        "\n",
        "datos_textblob.conversacion2.posTaggingPre = TextBlob(conversacionElIdiota2).pos_tags\n",
        "datos_textblob.conversacion2.posTaggingPost = normalizar_textBlob(conversacionElIdiota2,True).pos_tags"
      ],
      "metadata": {
        "id": "yv1U_Zj0Wget"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_textblob.imprimePosTagging()"
      ],
      "metadata": {
        "id": "_YGyZiUDXfBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultado de procesamiento como cadena"
      ],
      "metadata": {
        "id": "WtREAvuiXns8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_textblob.texto1.textoProcesado         = normalizar_textBlob(fragmentoElIdiota1,devolverCadena=True)\n",
        "datos_textblob.texto2.textoProcesado         = normalizar_textBlob(fragmentoElIdiota2,devolverCadena=True)\n",
        "datos_textblob.conversacion1.textoProcesado  = normalizar_textBlob(conversacionElIdiota1,devolverCadena=True)\n",
        "datos_textblob.conversacion2.textoProcesado  = normalizar_textBlob(conversacionElIdiota2,devolverCadena=True)"
      ],
      "metadata": {
        "id": "Q-VaIcgzXrfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_textblob.textosProcesados2file(\"TextBlob\")"
      ],
      "metadata": {
        "id": "Z_hjw9KtX16V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gensim"
      ],
      "metadata": {
        "id": "vRCfFsazX5tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalaciones"
      ],
      "metadata": {
        "id": "Uxtpi-ViYKeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install PatternLite\n",
        "!pip install autocorrect"
      ],
      "metadata": {
        "id": "65PD8WdwYRJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones y Clases\n"
      ],
      "metadata": {
        "id": "s-K0GyFDYMzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.parsing.preprocessing import (\n",
        "    preprocess_string, strip_tags, strip_punctuation, strip_numeric,\n",
        "    remove_stopwords, strip_short, stem_text\n",
        ")\n",
        "\n",
        "from pattern.en import parser\n",
        "from autocorrect import Speller"
      ],
      "metadata": {
        "id": "e43LghqGX5Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizar_gensim(texto,devolverCadena=False):\n",
        "  #Antes de lematizar, se corrige el texto\n",
        "  texto = Speller(lang=\"en\")(texto)\n",
        "\n",
        "\n",
        "  texto_anotado= parser.parse(texto,tokenize=False,tags=False,chunks=False,relations=False,lemmata=True)\n",
        "  lista_texto_sin_anotar =   [(palabra[2]) for linea  in texto_anotado.split() for palabra in linea]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Lista de filtros que para el preprocesado (de la guía oficial)\n",
        "  FILTROS_A_APLICAR = [\n",
        "    strip_punctuation,    # Remove punctuation\n",
        "    strip_numeric,        # Remove numbers\n",
        "    remove_stopwords,     # Remove common stopwords\n",
        "    strip_short,          # Remove words shorter than 3 characters (Parte de los stop-words)\n",
        "    stem_text             # Stem the words\n",
        "    ]\n",
        "\n",
        "  texto_unido=[]\n",
        "\n",
        "  for palabrasLinea in lista_texto_sin_anotar:\n",
        "    texto_unido = ' '.join(palabrasLinea)\n",
        "\n",
        "\n",
        "  texto_unido = ' '.join(lista_texto_sin_anotar)\n",
        "\n",
        "\n",
        "  # Se normaliza el texto\n",
        "  texto_preprocesado = preprocess_string(texto_unido, FILTROS_A_APLICAR)\n",
        "\n",
        "  salida = texto_preprocesado if not devolverCadena else ' '.join(texto_preprocesado)\n",
        "\n",
        "  return salida"
      ],
      "metadata": {
        "id": "vH_Czs4MYQlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del almacen de datos"
      ],
      "metadata": {
        "id": "IUiv6D42YwhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_gensim = almacen_datos()"
      ],
      "metadata": {
        "id": "Yqin_XbpY0yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TestBench"
      ],
      "metadata": {
        "id": "l8DzY8oxYlZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_gensim.texto1.tiempoProcesado        = %timeit -r 100 -o normalizar_gensim(fragmentoElIdiota1)\n",
        "datos_gensim.texto2.tiempoProcesado        = %timeit -r 100 -o normalizar_gensim(fragmentoElIdiota2)\n",
        "datos_gensim.conversacion1.tiempoProcesado = %timeit -r 100 -o normalizar_gensim(conversacionElIdiota1)\n",
        "datos_gensim.conversacion2.tiempoProcesado = %timeit -r 100 -o normalizar_gensim(conversacionElIdiota1)\n"
      ],
      "metadata": {
        "id": "ofxD0LXZYpXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Descripcion 1: Media:{EngNumber(datos_gensim.texto1.tiempoProcesado.average)}                     | desv_tipica: {EngNumber(datos_gensim.texto1.tiempoProcesado.stdev)}                     | Mejor: {EngNumber(datos_gensim.texto1.tiempoProcesado.best)}                     | Peor: {EngNumber(datos_gensim.texto1.tiempoProcesado.worst)}        | tiempo de compilación: {EngNumber(datos_gensim.texto1.tiempoProcesado.compile_time)}')\n",
        "print(f'Descripcion 2: Media:{EngNumber(datos_gensim.texto2.tiempoProcesado.average)}                     | desv_tipica: {EngNumber(datos_gensim.texto2.tiempoProcesado.stdev)}                     | Mejor: {EngNumber(datos_gensim.texto2.tiempoProcesado.best)}                     | Peor: {EngNumber(datos_gensim.texto2.tiempoProcesado.worst)}        | tiempo de compilación: {EngNumber(datos_gensim.texto2.tiempoProcesado.compile_time)}')\n",
        "print(f'Conversacion 1: Media:{EngNumber(datos_gensim.conversacion1.tiempoProcesado.average)}              | desv_tipica: {EngNumber(datos_gensim.conversacion1.tiempoProcesado.stdev)}             | Mejor: {EngNumber(datos_gensim.conversacion1.tiempoProcesado.best)}             | Peor: {EngNumber(datos_gensim.conversacion1.tiempoProcesado.worst)}  | tiempo de compilación: {EngNumber(datos_gensim.conversacion1.tiempoProcesado.compile_time)}')\n",
        "print(f'Conversacion 2: Media:{EngNumber(datos_gensim.conversacion2.tiempoProcesado.average)}              | desv_tipica: {EngNumber(datos_gensim.conversacion2.tiempoProcesado.stdev)}             | Mejor: {EngNumber(datos_gensim.conversacion2.tiempoProcesado.best)})            | Peor: {EngNumber(datos_gensim.conversacion2.tiempoProcesado.worst)}  | tiempo de compilación: {EngNumber(datos_gensim.conversacion2.tiempoProcesado.compile_time)}')"
      ],
      "metadata": {
        "id": "koKPmg3yZn3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag Of Words (BoW)"
      ],
      "metadata": {
        "id": "QSu-NlmfbEw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "\n",
        "def bowizar(texto):\n",
        "  diccionario = corpora.Dictionary([texto])\n",
        "\n",
        "  # bag of words\n",
        "  bow = diccionario.doc2bow(texto, allow_update=True)\n",
        "\n",
        "  # Consigo la id\n",
        "  token2id = diccionario.token2id\n",
        "\n",
        "  lista_doc = [(list(token2id.keys())[list(token2id.values()).index(id)],freq) for (id,freq) in bow]\n",
        "  return lista_doc\n",
        "\n",
        "\n",
        "t1Previo = [(palabra[2]) for linea in parser.parse(fragmentoElIdiota1,tokenize=False,tags=False,chunks=False,relations=False,lemmata=True).split() for palabra in linea]\n",
        "t1Posterior = normalizar_gensim(fragmentoElIdiota1)\n",
        "t2Previo = [(palabra[2]) for linea in parser.parse(fragmentoElIdiota2,tokenize=False,tags=False,chunks=False,relations=False,lemmata=True).split() for palabra in linea]\n",
        "t2Posterior = normalizar_gensim(fragmentoElIdiota2)\n",
        "t3Previo = [(palabra[2]) for linea in parser.parse(conversacionElIdiota1,tokenize=False,tags=False,chunks=False,relations=False,lemmata=True).split() for palabra in linea]\n",
        "t3Posterior = normalizar_gensim(conversacionElIdiota1)\n",
        "t4Previo = [(palabra[2]) for linea in parser.parse(conversacionElIdiota2,tokenize=False,tags=False,chunks=False,relations=False,lemmata=True).split() for palabra in linea]\n",
        "t4Posterior = normalizar_gensim(conversacionElIdiota2)\n",
        "\n",
        "datos_gensim.texto1.bowPrevio = bowizar(t1Previo)\n",
        "datos_gensim.texto1.bowPosterior = bowizar(t1Posterior)\n",
        "\n",
        "datos_gensim.texto2.bowPrevio = bowizar(t2Previo)\n",
        "datos_gensim.texto2.bowPosterior = bowizar(t2Posterior)\n",
        "\n",
        "datos_gensim.conversacion1.bowPrevio = bowizar(t3Previo)\n",
        "datos_gensim.conversacion1.bowPosterior = bowizar(t3Posterior)\n",
        "\n",
        "datos_gensim.conversacion2.bowPrevio = bowizar(t4Previo)\n",
        "datos_gensim.conversacion2.bowPosterior = bowizar(t4Posterior)"
      ],
      "metadata": {
        "id": "Jh_D8tM_bIm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_gensim.imprimeTodosLosBows()"
      ],
      "metadata": {
        "id": "MpAcoIIZczVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentimientos"
      ],
      "metadata": {
        "id": "sgo-4ZUqdBcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto1 = \" \".join(normalizar_gensim(fragmentoElIdiota1))\n",
        "texto2 = \" \".join(normalizar_gensim(fragmentoElIdiota2))\n",
        "convo1 = \" \".join(normalizar_gensim(conversacionElIdiota1))\n",
        "convo2 = \" \".join(normalizar_gensim(conversacionElIdiota2))\n",
        "\n",
        "datos_gensim.texto1.sentimientosPrevios = TextBlob(fragmentoElIdiota1).sentiment\n",
        "datos_gensim.texto2.sentimientosPrevios = TextBlob(fragmentoElIdiota2).sentiment\n",
        "datos_gensim.conversacion1.sentimientosPrevios = TextBlob(conversacionElIdiota1).sentiment\n",
        "datos_gensim.conversacion2.sentimientosPrevios = TextBlob(conversacionElIdiota2).sentiment\n",
        "\n",
        "datos_gensim.texto1.sentimientosPosteriores = TextBlob(texto1).sentiment\n",
        "datos_gensim.texto2.sentimientosPosteriores = TextBlob(texto2).sentiment\n",
        "datos_gensim.conversacion1.sentimientosPosteriores = TextBlob(convo1).sentiment\n",
        "datos_gensim.conversacion2.sentimientosPosteriores = TextBlob(convo2).sentiment\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8NqmA6bNdDFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_gensim.imprimeSentimientos()"
      ],
      "metadata": {
        "id": "tlvIjFEMddzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados de Procesamiento como cadena"
      ],
      "metadata": {
        "id": "5gsPG46rdi1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_gensim.texto1.textoProcesado = normalizar_gensim(pruebaElIdiota1,devolverCadena=True)\n",
        "datos_gensim.texto2.textoProcesado = normalizar_gensim(pruebaElIdiota2,devolverCadena=True)\n",
        "datos_gensim.conversacion1.textoProcesado = normalizar_gensim(conversacionElIdiota1,devolverCadena=True)\n",
        "datos_gensim.conversacion2.textoProcesado = normalizar_gensim(conversacionElIdiota2,devolverCadena=True)"
      ],
      "metadata": {
        "id": "vzHQPBH8XptU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_gensim.textosProcesados2file(\"Gensim\")"
      ],
      "metadata": {
        "id": "wScr2zAHdzFQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}