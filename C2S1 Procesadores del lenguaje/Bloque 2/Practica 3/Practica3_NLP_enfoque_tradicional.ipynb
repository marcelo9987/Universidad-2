{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {
    "id": "0oFc6yQWgalf"
   },
   "cell_type": "markdown",
   "source": [
    "# NLP - Enfoque tradicional\n",
    "Este notebook cubre varios conceptos del procesamiento del lenguaje natural (NLP) tradicional y utiliza diferentes bibliotecas como NLTK, SpaCy, TextBlob, Gensim y Scikit-learn para implementarlos y visualizarlos.\n"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqRKuoCug2yN",
    "outputId": "f040ecce-4355-4893-d798-e0c6746b7323",
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:06:15.024695Z",
     "start_time": "2024-11-18T15:05:22.468130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install nltk                   --user\n",
    "!pip install spacy                  --user\n",
    "!pip install textblob               --user\n",
    "!pip install gensim                 --user\n",
    "!pip install scikit-learn           --user\n",
    "!pip install pyLDAvis               --user\n",
    "!pip install language_tool_python   --user"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.10-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python312\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Using cached thinc-8.3.2-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.4.8-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.13.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python312\\lib\\site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from spacy) (75.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.4.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.23.4-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached blis-1.0.1-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached spacy-3.8.2-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.10-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached srsly-2.4.8-cp312-cp312-win_amd64.whl (478 kB)\n",
      "Using cached thinc-8.3.2-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl (15.6 MB)\n",
      "Using cached typer-0.13.0-py3-none-any.whl (44 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blis-1.0.1-cp312-cp312-win_amd64.whl (6.4 MB)\n",
      "Using cached cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.0.1 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 langcodes-3.4.1 language-data-1.2.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 numpy-2.0.2 preshed-3.0.9 pydantic-2.9.2 pydantic-core-2.23.4 rich-13.9.4 spacy-3.8.2 srsly-2.4.8 thinc-8.3.2 typer-0.13.0 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\imarc\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'C:\\Users\\imarc\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script typer.exe is installed in 'C:\\Users\\imarc\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script weasel.exe is installed in 'C:\\Users\\imarc\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\imarc\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\python312\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\python312\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (4.67.0)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\python312\\lib\\site-packages (4.3.3)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\python312\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python312\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\imarc\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\imarc\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\python312\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\imarc\\appdata\\roaming\\python\\python312\\site-packages (from pyLDAvis) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (from pyLDAvis) (1.13.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\python312\\lib\\site-packages (from pyLDAvis) (2.2.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from pyLDAvis) (1.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from pyLDAvis) (3.1.4)\n",
      "Requirement already satisfied: numexpr in c:\\python312\\lib\\site-packages (from pyLDAvis) (2.10.1)\n",
      "Requirement already satisfied: funcy in c:\\python312\\lib\\site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\python312\\lib\\site-packages (from pyLDAvis) (1.5.2)\n",
      "Requirement already satisfied: gensim in c:\\python312\\lib\\site-packages (from pyLDAvis) (4.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from pyLDAvis) (75.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python312\\lib\\site-packages (from gensim->pyLDAvis) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->pyLDAvis) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: wrapt in c:\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting language_tool_python\n",
      "  Using cached language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pip in c:\\python312\\lib\\site-packages (from language_tool_python) (24.3.1)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from language_tool_python) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from language_tool_python) (4.67.0)\n",
      "Requirement already satisfied: wheel in c:\\python312\\lib\\site-packages (from language_tool_python) (0.45.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->language_tool_python) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->language_tool_python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->language_tool_python) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->language_tool_python) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm->language_tool_python) (0.4.6)\n",
      "Using cached language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: language_tool_python\n",
      "Successfully installed language_tool_python-2.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcKGlXc1gali"
   },
   "source": [
    "## 1. Tokenización y Etiquetado POS (Análisis Léxico y Sintáctico - NLTK y SpaCy)\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Teoría:**\\\n",
    "La **tokenización** es el proceso de dividir un texto en unidades más pequeñas llamadas \"tokens\". Esto es importante porque la mayoría de las tareas de NLP necesitan trabajar con palabras individuales o grupos pequeños de palabras.\\\n",
    "El **etiquetado POS (Part-of-Speech)** clasifica cada palabra en su categoría gramatical (como sustantivo, verbo, adjetivo, etc.), lo cual es crucial para entender la estructura gramatical de una oración."
   ],
   "metadata": {
    "id": "8uKuoxgKhtAZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementación con NLTK**\n",
    "\n"
   ],
   "metadata": {
    "id": "TKH5ACm4hxlZ"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oH6qsGw2galk",
    "outputId": "e0886ab6-b64e-490e-80a2-a1fdad378d9b",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Ejemplo de texto en inglés\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenización - Léxico\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Etiquetado POS con NLTK - Sintáctico\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"Etiquetas POS:\", pos_tags)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge8Tt91Tgalm"
   },
   "source": [
    "**Visualización con Spacy**\\\n",
    "SpaCy proporciona una representación visual interactiva del **árbol de dependencias**, mostrando las **relaciones gramaticales** y las **etiquetas POS** *texto en cursiva*, lo que facilita la comprensión de la estructura de la oración."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "'''Necesitamos el modelo en español porque cada idioma tiene su propia estructura gramatical y reglas sintácticas.\n",
    "   Un modelo preentrenado en español como el de SpaCy puede realizar estas tareas de etiquetado gramatical y análisis sintáctico con precisión,\n",
    "   porque ha sido entrenado en los patrones lingüísticos específicos de este idioma.'''\n",
    "!python -m spacy download es_core_news_sm #decarga el modelo español de spacy (terminal)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "r5Gb4IwFjm0E",
    "outputId": "18fd6273-739e-4f79-8094-d089dc6fd449"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting es-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.9/12.9 MB\u001B[0m \u001B[31m63.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.12.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (71.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.16.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "yycE3iEEgalm",
    "outputId": "b80b7f30-7003-4f2d-f98f-5fa9444d515e"
   },
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Cargar el modelo en español\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = 'The quick brown fox jumps over the lazy dog.'\n",
    "\n",
    "# Procesar el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Visualizar el árbol de dependencias, incluyendo etiquetas POS\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})\n"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2b5ce45d9329431e9f668abdffff5c72-0\" class=\"displacy\" width=\"950\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">quick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">fox</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">jumps</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">dog.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2b5ce45d9329431e9f668abdffff5c72-0-0\" stroke-width=\"2px\" d=\"M70,152.0 C70,2.0 350.0,2.0 350.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2b5ce45d9329431e9f668abdffff5c72-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,154.0 L62,142.0 78,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2b5ce45d9329431e9f668abdffff5c72-0-1\" stroke-width=\"2px\" d=\"M170,152.0 C170,52.0 345.0,52.0 345.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2b5ce45d9329431e9f668abdffff5c72-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M170,154.0 L162,142.0 178,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2b5ce45d9329431e9f668abdffff5c72-0-2\" stroke-width=\"2px\" d=\"M270,152.0 C270,102.0 340.0,102.0 340.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2b5ce45d9329431e9f668abdffff5c72-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M270,154.0 L262,142.0 278,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2b5ce45d9329431e9f668abdffff5c72-0-3\" stroke-width=\"2px\" d=\"M370,152.0 C370,102.0 440.0,102.0 440.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2b5ce45d9329431e9f668abdffff5c72-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M370,154.0 L362,142.0 378,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2b5ce45d9329431e9f668abdffff5c72-0-4\" stroke-width=\"2px\" d=\"M470,152.0 C470,102.0 540.0,102.0 540.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2b5ce45d9329431e9f668abdffff5c72-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M540.0,154.0 L548.0,142.0 532.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2b5ce45d9329431e9f668abdffff5c72-0-5\" stroke-width=\"2px\" d=\"M670,152.0 C670,52.0 845.0,52.0 845.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2b5ce45d9329431e9f668abdffff5c72-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,154.0 L662,142.0 678,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2b5ce45d9329431e9f668abdffff5c72-0-6\" stroke-width=\"2px\" d=\"M770,152.0 C770,102.0 840.0,102.0 840.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2b5ce45d9329431e9f668abdffff5c72-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,154.0 L762,142.0 778,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2b5ce45d9329431e9f668abdffff5c72-0-7\" stroke-width=\"2px\" d=\"M570,152.0 C570,2.0 850.0,2.0 850.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2b5ce45d9329431e9f668abdffff5c72-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M850.0,154.0 L858.0,142.0 842.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk4PTycYgaln"
   },
   "source": [
    "## 2. Lematización y Reconocimiento de Entidades Nombradas (NER) (Análisis Léxico y Semántico - SpaCy)\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Teoría:**\\\n",
    "La **lematización** reduce una palabra a su forma base o lema, ayudando a agrupar palabras con el mismo significado pero diferentes formas morfológicas.\\\n",
    "El **Reconocimiento de Entidades Nombradas (NER)** identifica entidades clave en el texto, como personas, lugares, organizaciones, etc., y es útil para extraer información relevante automáticamente."
   ],
   "metadata": {
    "id": "esWHgj3vlM6J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementación y visualización con SpaCy.**"
   ],
   "metadata": {
    "id": "aoc9uAc3lPIT"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PHUcLBYagaln",
    "outputId": "63532d06-ebe5-4539-ecf0-12a86c7b4414"
   },
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "!python -m spacy download es_core_news_sm\n",
    "# Cargar modelo en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = 'Apple fue fundada por Steve Jobs en California.'\n",
    "\n",
    "# Procesar el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Lematización - Léxico\n",
    "print('Lemas:')\n",
    "for token in doc:\n",
    "    print(f'{token.text} -> {token.lemma_}')\n",
    "\n",
    "# Visualización de Entidades Nombradas - léxico/semántico\n",
    "displacy.render(doc, style='ent', jupyter=True)\n"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting es-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.9/12.9 MB\u001B[0m \u001B[31m50.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.13.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.7.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Lemas:\n",
      "Apple -> Apple\n",
      "fue -> ser\n",
      "fundada -> fundar\n",
      "por -> por\n",
      "Steve -> Steve\n",
      "Jobs -> Jobs\n",
      "en -> en\n",
      "California -> California\n",
      ". -> .\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " fue fundada por \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " en \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "IuINyxEYlnq5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "CywoznsRsV4z"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJAX3owwlvQ-"
   },
   "source": [
    "## 3. Análisis de Sentimiento y Corrección Gramatical (Análisis Semántico - TextBlob)\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Teoría:**\\\n",
    "El **análisis de sentimiento** mide la polaridad del texto (positivo, negativo, neutral) y es útil para comprender la opinión o actitud expresada en grandes volúmenes de texto.\\\n",
    "La **corrección gramatical** asegura la claridad y precisión del texto, especialmente en aplicaciones de escritura asistida."
   ],
   "metadata": {
    "id": "XFdhcGddl6m6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementación con TextBlob. - Analisis de sentimiento**"
   ],
   "metadata": {
    "id": "00NQB7M_l62A"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QSDCvxH9lvRA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "outputId": "fb3016aa-f6f3-4ac5-a22c-27f54cb2901c"
   },
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Example text\n",
    "text = 'This product is absolutely wonderful.'\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment = blob.sentiment\n",
    "print(f'Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}')\n",
    "'''\n",
    "La frase tiene un sentimiento extremadamente positivo,\n",
    "por lo que la polaridad debería estar cerca de 1.0.\n",
    "Dado que la frase expresa una opinión subjetiva sobre el producto,\n",
    "la subjetividad también debería ser alta, cerca de 1.0\n",
    "'''"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Polarity: 1.0, Subjectivity: 1.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nLa frase tiene un sentimiento extremadamente positivo,\\npor lo que la polaridad debería estar cerca de 1.0.\\nDado que la frase expresa una opinión subjetiva sobre el producto,\\nla subjetividad también debería ser alta, cerca de 1.0\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementación con Language Tool. - Corrección gramatical**"
   ],
   "metadata": {
    "id": "8-Q5RKPgr6cp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip install language_tool_python\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "text_with_errors = \"She go to the market and buy apples.\"\n",
    "\n",
    "# Aplicar la corrección\n",
    "matches = tool.check(text_with_errors)\n",
    "corrected_text = language_tool_python.utils.correct(text_with_errors, matches)\n",
    "\n",
    "print(\"Original text:\", text_with_errors)\n",
    "print(\"Corrected text:\", corrected_text)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2v3H6STnvnP",
    "outputId": "1228031e-9c22-4305-e20d-441854e7fd2f"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading LanguageTool 6.4: 100%|██████████| 246M/246M [00:04<00:00, 55.4MB/s]\n",
      "INFO:language_tool_python.download_lt:Unzipping /tmp/tmp_8ikm8fy.zip to /root/.cache/language_tool_python.\n",
      "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-6.4.zip to /root/.cache/language_tool_python.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original text: She go to the market and buy apples.\n",
      "Corrected text: She goes to the market and buy apples.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Bolsa de Palabras (BoW) y TF-IDF (Análisis Léxico - Gensim y Scikit-learn)\n",
    "----------------"
   ],
   "metadata": {
    "id": "zos_Hqp0sohD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Teoría:**\\\n",
    "La **Bolsa de Palabras (BoW)** representa un documento como un vector de frecuencias de palabras, ignorando el orden.\\\n",
    "El **TF-IDF** pondera la importancia de una palabra en un documento en relación con el conjunto de documentos, destacando palabras relevantes mientras se penalizan las comunes."
   ],
   "metadata": {
    "id": "PxaNA1xzsu1x"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementacion con Gensim - BoW**"
   ],
   "metadata": {
    "id": "TmGH1o_Gs_dS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Gensim para Bolsa de Palabras (BoW)\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Documentos de ejemplo\n",
    "documents = ['El gato negro saltó sobre el sofá.', 'El perro ladró fuertemente en la casa.']\n",
    "\n",
    "# Tokenización\n",
    "texts = [[word.lower() for word in document.split()] for document in documents]\n",
    "\n",
    "# Creación del diccionario\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(\"Diccionario:\", dictionary.token2id)\n",
    "\n",
    "#Creación de la bolsa de palabras\n",
    "corpus_bow = [dictionary.doc2bow(text) for text in texts]\n",
    "print('Bolsa de Palabras:', corpus_bow)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z41ZO_oGtABS",
    "outputId": "5e48a711-9f45-4335-8001-806e88bda492"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Diccionario: {'el': 0, 'gato': 1, 'negro': 2, 'saltó': 3, 'sobre': 4, 'sofá.': 5, 'casa.': 6, 'en': 7, 'fuertemente': 8, 'la': 9, 'ladró': 10, 'perro': 11}\n",
      "Bolsa de Palabras: [[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)], [(0, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "El diccionario representa el indice asignado a cada palabra.\\\n",
    "En la Bolsa de Palabras, tenemos un array por cada documento (frase en este caso). Cada array contiene una tupla (pares) por cada palabra. El primer numero de la tupla indica el indice de la palabra, el segundo cuantas veces se repite **en ese mismo** documento.\\\n",
    "\\\n",
    "Una representación más grafica de la bolsa de palabras:"
   ],
   "metadata": {
    "id": "yIpworsj763c"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Convertir corpus_bow a una representación más visual utilizando pandas DataFrame\n",
    "# Creamos una matriz donde las columnas serán las palabras y las filas serán los documentos\n",
    "import pandas as pd\n",
    "data = []\n",
    "\n",
    "for bow in corpus_bow:\n",
    "    bow_dict = dict(bow)\n",
    "    data.append([bow_dict.get(dictionary.token2id[word], 0) for word in dictionary.token2id])\n",
    "\n",
    "# Crear un DataFrame con las palabras como columnas y \"doc 1\", \"doc 2\" como índices\n",
    "doc_names = [f\"doc {i+1}\" for i in range(len(corpus_bow))]\n",
    "df_bow = pd.DataFrame(data, columns=[dictionary[id] for id in range(len(dictionary))], index=doc_names)\n",
    "\n",
    "# Estilo de la tabla con líneas delimitadoras\n",
    "styled_df_bow = df_bow.style.set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('border', '1px solid black')]},\n",
    "     {'selector': 'td', 'props': [('border', '1px solid black')]}]\n",
    ").set_properties(**{'text-align': 'center'})\n",
    "\n",
    "# Mostrar la tabla estilizada\n",
    "styled_df_bow"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "id": "AOwUsSZCtFYl",
    "outputId": "f263d60b-9e51-4049-a300-347926736e5d"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7e979ff5e950>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_16a17 th {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_16a17 td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_16a17_row0_col0, #T_16a17_row0_col1, #T_16a17_row0_col2, #T_16a17_row0_col3, #T_16a17_row0_col4, #T_16a17_row0_col5, #T_16a17_row0_col6, #T_16a17_row0_col7, #T_16a17_row0_col8, #T_16a17_row0_col9, #T_16a17_row0_col10, #T_16a17_row0_col11, #T_16a17_row1_col0, #T_16a17_row1_col1, #T_16a17_row1_col2, #T_16a17_row1_col3, #T_16a17_row1_col4, #T_16a17_row1_col5, #T_16a17_row1_col6, #T_16a17_row1_col7, #T_16a17_row1_col8, #T_16a17_row1_col9, #T_16a17_row1_col10, #T_16a17_row1_col11 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_16a17\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_16a17_level0_col0\" class=\"col_heading level0 col0\" >el</th>\n",
       "      <th id=\"T_16a17_level0_col1\" class=\"col_heading level0 col1\" >gato</th>\n",
       "      <th id=\"T_16a17_level0_col2\" class=\"col_heading level0 col2\" >negro</th>\n",
       "      <th id=\"T_16a17_level0_col3\" class=\"col_heading level0 col3\" >saltó</th>\n",
       "      <th id=\"T_16a17_level0_col4\" class=\"col_heading level0 col4\" >sobre</th>\n",
       "      <th id=\"T_16a17_level0_col5\" class=\"col_heading level0 col5\" >sofá.</th>\n",
       "      <th id=\"T_16a17_level0_col6\" class=\"col_heading level0 col6\" >casa.</th>\n",
       "      <th id=\"T_16a17_level0_col7\" class=\"col_heading level0 col7\" >en</th>\n",
       "      <th id=\"T_16a17_level0_col8\" class=\"col_heading level0 col8\" >fuertemente</th>\n",
       "      <th id=\"T_16a17_level0_col9\" class=\"col_heading level0 col9\" >la</th>\n",
       "      <th id=\"T_16a17_level0_col10\" class=\"col_heading level0 col10\" >ladró</th>\n",
       "      <th id=\"T_16a17_level0_col11\" class=\"col_heading level0 col11\" >perro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_16a17_level0_row0\" class=\"row_heading level0 row0\" >doc 1</th>\n",
       "      <td id=\"T_16a17_row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "      <td id=\"T_16a17_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_16a17_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "      <td id=\"T_16a17_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_16a17_row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "      <td id=\"T_16a17_row0_col5\" class=\"data row0 col5\" >1</td>\n",
       "      <td id=\"T_16a17_row0_col6\" class=\"data row0 col6\" >0</td>\n",
       "      <td id=\"T_16a17_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "      <td id=\"T_16a17_row0_col8\" class=\"data row0 col8\" >0</td>\n",
       "      <td id=\"T_16a17_row0_col9\" class=\"data row0 col9\" >0</td>\n",
       "      <td id=\"T_16a17_row0_col10\" class=\"data row0 col10\" >0</td>\n",
       "      <td id=\"T_16a17_row0_col11\" class=\"data row0 col11\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16a17_level0_row1\" class=\"row_heading level0 row1\" >doc 2</th>\n",
       "      <td id=\"T_16a17_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_16a17_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_16a17_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_16a17_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_16a17_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_16a17_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "      <td id=\"T_16a17_row1_col6\" class=\"data row1 col6\" >1</td>\n",
       "      <td id=\"T_16a17_row1_col7\" class=\"data row1 col7\" >1</td>\n",
       "      <td id=\"T_16a17_row1_col8\" class=\"data row1 col8\" >1</td>\n",
       "      <td id=\"T_16a17_row1_col9\" class=\"data row1 col9\" >1</td>\n",
       "      <td id=\"T_16a17_row1_col10\" class=\"data row1 col10\" >1</td>\n",
       "      <td id=\"T_16a17_row1_col11\" class=\"data row1 col11\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementacion con Scikit-learn - TF-IDF**"
   ],
   "metadata": {
    "id": "1gIF5U10-P_H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Frecuencia de Término (TF):** Mide cuántas veces aparece una palabra específica en **un** documento.\n",
    "\n",
    "**Frecuencia Inversa de Documentos (IDF):** Mide la rareza de una palabra en el conjunto de documentos. Si una palabra aparece en muchos documentos, su IDF será **baja**, porque es **menos informativa** (palabras comunes como \"el\", \"y\", \"es\").\\\n",
    "**La ponderación TF-IDF**: se calcula multiplicando la TF de la palabra por su IDF. Esto pondera la frecuencia de la palabra según su rareza en el conjunto de documentos.\n",
    "\\\n",
    "**Casos de uso:** Clasificación de Textos, Búsqueda de Información, Filtrado de Palabras Relevantes"
   ],
   "metadata": {
    "id": "CC3crcJt_DZz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Scikit-learn para TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "documents = [\n",
    "    \"El gato negro saltó sobre el sofá. Luego, el gato descansó cómodamente en su lugar favorito. El gato siempre salta con agilidad.\",\n",
    "    \"El perro ladró fuertemente en la casa. Después, el perro salió a jugar en el jardín. El perro corre rápidamente.\",\n",
    "    \"El gato cazó un ratón en el jardín. Los gatos son excelentes cazadores, siempre al acecho. El gato volvió al sofá.\",\n",
    "    \"El perro dormía plácidamente en su cama. Cuando el perro se despertó, salió corriendo hacia el parque. El perro ama los paseos.\",\n",
    "    \"Los gatos son animales muy independientes. Les gusta dormir durante el día y cazar por la noche. El gato siempre vuelve a casa.\",\n",
    "    \"El perro es conocido por su lealtad hacia los humanos. El perro cuida la casa y siempre está alerta ante cualquier ruido extraño.\",\n",
    "    \"El gato se subió al árbol para escapar del perro. Los gatos son conocidos por su capacidad de trepar y escapar del peligro.\",\n",
    "    \"La computadora se apagó repentinamente mientras estaba ejecutando un programa importante. Después de reiniciarla, todos los archivos volvieron a estar accesibles.\"\n",
    "]\n",
    "\n",
    "# Crear y ajustar el vectorizador TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(documents)\n",
    "\n",
    "# Obtener los nombres de las palabras (vocabulario)\n",
    "feature_names = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "# Convertir la matriz TF-IDF a un DataFrame de pandas\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=feature_names)\n",
    "\n",
    "# Añadir nombres de documentos (opcional)\n",
    "df_tfidf.index = [\n",
    "    '1 (gatos)',\n",
    "    '2 (perros)',\n",
    "    '3 (gatos)',\n",
    "    '4 (perros)',\n",
    "    '5 (gatos)',\n",
    "    '6 (perros)',\n",
    "    '7 (gatos)',\n",
    "    '8 (computadoras)'\n",
    "]\n",
    "# Mostrar la tabla\n",
    "print(df_tfidf)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrGWRG_Mvdac",
    "outputId": "24294e70-ec39-4e00-ff9c-46baaa7ea0a0"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                  accesibles    acecho  agilidad        al   alerta       ama  \\\n",
      "1 (gatos)           0.000000  0.000000  0.225216  0.000000  0.00000  0.000000   \n",
      "2 (perros)          0.000000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
      "3 (gatos)           0.000000  0.246797  0.000000  0.413670  0.00000  0.000000   \n",
      "4 (perros)          0.000000  0.000000  0.000000  0.000000  0.00000  0.230705   \n",
      "5 (gatos)           0.000000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
      "6 (perros)          0.000000  0.000000  0.000000  0.000000  0.24537  0.000000   \n",
      "7 (gatos)           0.000000  0.000000  0.000000  0.186692  0.00000  0.000000   \n",
      "8 (computadoras)    0.240549  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
      "\n",
      "                  animales     ante     apagó  archivos  ...       son  \\\n",
      "1 (gatos)          0.00000  0.00000  0.000000  0.000000  ...  0.000000   \n",
      "2 (perros)         0.00000  0.00000  0.000000  0.000000  ...  0.000000   \n",
      "3 (gatos)          0.00000  0.00000  0.000000  0.000000  ...  0.178482   \n",
      "4 (perros)         0.00000  0.00000  0.000000  0.000000  ...  0.000000   \n",
      "5 (gatos)          0.25528  0.00000  0.000000  0.000000  ...  0.184617   \n",
      "6 (perros)         0.00000  0.24537  0.000000  0.000000  ...  0.000000   \n",
      "7 (gatos)          0.00000  0.00000  0.000000  0.000000  ...  0.161100   \n",
      "8 (computadoras)   0.00000  0.00000  0.240549  0.240549  ...  0.000000   \n",
      "\n",
      "                        su     subió     todos    trepar        un  volvieron  \\\n",
      "1 (gatos)         0.142805  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "2 (perros)        0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "3 (gatos)         0.000000  0.000000  0.000000  0.000000  0.206835   0.000000   \n",
      "4 (perros)        0.146285  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "5 (gatos)         0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "6 (perros)        0.155584  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "7 (gatos)         0.141249  0.222762  0.000000  0.222762  0.000000   0.000000   \n",
      "8 (computadoras)  0.000000  0.000000  0.240549  0.000000  0.201599   0.240549   \n",
      "\n",
      "                    volvió   vuelve     árbol  \n",
      "1 (gatos)         0.000000  0.00000  0.000000  \n",
      "2 (perros)        0.000000  0.00000  0.000000  \n",
      "3 (gatos)         0.246797  0.00000  0.000000  \n",
      "4 (perros)        0.000000  0.00000  0.000000  \n",
      "5 (gatos)         0.000000  0.25528  0.000000  \n",
      "6 (perros)        0.000000  0.00000  0.000000  \n",
      "7 (gatos)         0.000000  0.00000  0.222762  \n",
      "8 (computadoras)  0.000000  0.00000  0.000000  \n",
      "\n",
      "[8 rows x 97 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejemplo de motor de búsqueda\n",
    "**Consulta:** El usuario ingresa una consulta, en este caso \"gato sofá\".\\\n",
    "**Transformación:** La consulta se transforma en su vector TF-IDF utilizando el mismo vectorizador que ya entrenaste con los documentos.\n",
    "**Similitud de Coseno:** Calculamos la similitud de coseno entre el vector TF-IDF de la consulta y los vectores de TF-IDF de los documentos.\\\n",
    "* Similitud de Coseno: mide cuán similares son dos vectores (en este caso, el vector de la consulta y los vectores de los documentos).\\\n",
    "* El valor varía entre 0 y 1, donde 1 significa documentos idénticos.\\\n",
    "\n",
    "**Resultado:** Mostramos las similitudes de coseno para cada documento y resaltamos cuál es el más relevante."
   ],
   "metadata": {
    "id": "W3b9IFC1FVsR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Tu consulta\n",
    "query = \"gato sofá\"\n",
    "\n",
    "# Normalizar la consulta usando el mismo vectorizador TF-IDF que ya has creado\n",
    "query_tfidf = vectorizer_tfidf.transform([query])\n",
    "\n",
    "# Calcular la similitud de coseno entre la consulta y cada documento\n",
    "cosine_similarities = cosine_similarity(query_tfidf, X_tfidf).flatten()\n",
    "\n",
    "# Mostrar las similitudes\n",
    "print(\"Similitud de coseno entre la consulta y los documentos:\")\n",
    "for idx, sim in enumerate(cosine_similarities):\n",
    "    print(f\"Documento {idx + 1}: {sim}\")\n",
    "\n",
    "# Encontrar el documento más relevante\n",
    "most_similar_doc_index = np.argmax(cosine_similarities)\n",
    "print(f\"\\nEl documento más relevante para la consulta '{query}' es el Documento {most_similar_doc_index + 1}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LxjktLqA47c",
    "outputId": "9eabaf7a-503a-40dc-8d9c-6aa1029a16f4"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similitud de coseno entre la consulta y los documentos:\n",
      "Documento 1: 0.4090089090150796\n",
      "Documento 2: 0.0\n",
      "Documento 3: 0.3537824405001258\n",
      "Documento 4: 0.0\n",
      "Documento 5: 0.09766441205287543\n",
      "Documento 6: 0.0\n",
      "Documento 7: 0.08522379566333722\n",
      "Documento 8: 0.0\n",
      "\n",
      "El documento más relevante para la consulta 'gato sofá' es el Documento 1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Practica"
   ],
   "metadata": {
    "id": "B_nUR2B1G1nT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ejercicio: Exploración de la Normalización en Distintas Librerías\n",
    "\n",
    "### Introducción:\n",
    "\n",
    "La normalización del texto es un paso esencial en el procesamiento del lenguaje natural (NLP) que asegura que el texto esté limpio y estructurado para que pueda ser procesado por los algoritmos de NLP. En este ejercicio, aprenderás a aplicar diferentes técnicas de normalización del texto utilizando las librerías que hemos visto hasta ahora: **NLTK**, **SpaCy**, **TextBlob**, y **Gensim**.\n",
    "\n",
    "### Técnicas de Normalización:\n",
    "\n",
    "1. **Lowercasing**: Convertir todo el texto a minúsculas.\n",
    "2. **Eliminar puntuación**: Eliminar signos de puntuación innecesarios.\n",
    "3. **Eliminar números**: Remover los números que no aporten valor al análisis.\n",
    "4. **Eliminar stop words**: Filtrar palabras comunes que no aportan información.\n",
    "5. **Lematización**: Reducir las palabras a su forma base.\n",
    "6. **Stemming (opcional)**: Aplicar stemming si la librería lo soporta.\n",
    "7. **Corrección ortográfica**: Corregir errores ortográficos en el texto.\n",
    "8. **Tokenización**: Dividir el texto en tokens.\n",
    "\n",
    "### Instrucciones:\n",
    "\n",
    "1. Busca en la documentación de cada librería (NLTK, SpaCy, TextBlob, Gensim) cómo puedes aplicar estas técnicas de normalización.\n",
    "2. Implementa las soluciones que encuentres para normalizar el texto en cada librería.\n",
    "3. Compara los resultados obtenidos en cada una:\n",
    " - ¿Qué diferencias encuentras entre las librerías?¿Cuáles son las fortalezas y limitaciones de cada una?\n",
    " - ¿Cómo afectó la normalización los resultados obtenidos en las diferentes técnicas de NLP (BoW, análisis de sentimiento, POS tagging, etc.)?\n",
    "4. Documenta lo implementado en cada técnica, el output que refleje los cambios y una comparativa antes y después del cambio.\n",
    "\n",
    "### Recursos:\n",
    "\n",
    "Aquí tienes enlaces a la documentación de cada librería para comenzar tu investigación:\n",
    "\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [SpaCy Documentation](https://spacy.io/usage)\n",
    "- [TextBlob Documentation](https://textblob.readthedocs.io/en/dev/)\n",
    "- [Gensim Documentation](https://radimrehurek.com/gensim/)"
   ],
   "metadata": {
    "id": "GXAFX5jWG_yq"
   }
  }
 ]
}
